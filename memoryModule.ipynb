{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/frattitamayo/.cache/huggingface/hub/models--NousResearch--Hermes-2-Pro-Mistral-7B-GGUF/snapshots/594e3e33f57a2b8693972e6bf48ae4eff404f170/Hermes-2-Pro-Mistral-7B.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = jeffq\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32032]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32032]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32032]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 291/32032 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32032\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = jeffq\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4893.18 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    10.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    72.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'jeffq', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
      "Guessed chat format: chatml\n",
      "\n",
      "llama_print_timings:        load time =     193.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     193.69 ms /     7 tokens (   27.67 ms per token,    36.14 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     194.03 ms /     8 tokens\n",
      "\n",
      "llama_print_timings:        load time =     193.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     196.90 ms /     7 tokens (   28.13 ms per token,    35.55 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     196.38 ms /     8 tokens\n",
      "\n",
      "llama_print_timings:        load time =     193.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     157.25 ms /     5 tokens (   31.45 ms per token,    31.80 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     157.48 ms /     6 tokens\n",
      "/home/frattitamayo/anaconda3/envs/llama2/lib/python3.10/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! max_new_tokens is not default parameter.\n",
      "                max_new_tokens was transferred to model_kwargs.\n",
      "                Please confirm that max_new_tokens is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/frattitamayo/.cache/huggingface/hub/models--NousResearch--Hermes-2-Pro-Mistral-7B-GGUF/snapshots/594e3e33f57a2b8693972e6bf48ae4eff404f170/Hermes-2-Pro-Mistral-7B.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = jeffq\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32032]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32032]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32032]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 291/32032 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32032\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = jeffq\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4893.18 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.20 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     2.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'jeffq', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
      "Guessed chat format: chatml\n"
     ]
    }
   ],
   "source": [
    "## Import functions\n",
    "import os\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import json\n",
    "from llm_demos.llama2.embeddings import llama_embed\n",
    "from llm_demos.llama2.gguf import importance_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama_embedding(document:str):\n",
    "    return llama_embed([document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Timestamp('2020-10-20 17:06:00'), 'Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Cassandra Winfred', 'Hello. '), (Timestamp('2020-10-20 17:06:00'), 'Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Cassandra Winfred', 'My assumption is'), (Timestamp('2020-10-20 17:06:00'), 'Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Cassandra Winfred ', 'that the emphasis on barbarism implies that she sent him to the lion.'), (Timestamp('2020-10-27 17:58:00'), 'Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Paige Tyrone', \"I agree with Cassandra's noticing \"), (Timestamp('2020-10-27 17:58:00'), 'Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Paige Tyrone', \"of the author's word choice of barbarism.\"), (Timestamp('2020-10-28 20:23:00'), 'Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Marissa Roswell', 'I loved the addition of '), (Timestamp('2020-10-28 20:23:00'), 'Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Marissa Roswell', 'Her lover would die and never love another.\" '), (Timestamp('2020-10-28 20:23:00'), 'Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Marissa Roswell', ' Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"'), (Timestamp('2020-11-11 12:59:00'), 'Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Paige Tyrone', 'Submitted')]\n"
     ]
    }
   ],
   "source": [
    "## Get data from files\n",
    "\n",
    "# Replace 'your_file_path.xlsm' with the path to your Excel file\n",
    "file_path = '/home/frattitamayo/memory_module/CodingDiscourseAnalysis/CollabWriteAnalysisTest.xlsm'\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(file_path, sheet_name='Test', parse_dates=['Message Time'])\n",
    "observations = []\n",
    "times = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # Construct the observation for each row\n",
    "    time_origin = row['Message Time']\n",
    "    origin = f\"Question: {row['Topic']}, Pseudonym: {row['Pseudonym']}\"\n",
    "    # origin = f\"{row['Pseudonym']}\"\n",
    "    value = row['Message']\n",
    "    observation = (time_origin, origin, value)\n",
    "     #observation = [(origin_1, value_1), (origin_2, value_2), ..., (origin_n, value_n)]\n",
    "    observations.append(observation)\n",
    "    \n",
    "print(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[0] Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Cassandra Winfred: \"Hello. \"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     193.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1905.30 ms /    73 tokens (   26.10 ms per token,    38.31 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1905.84 ms /    74 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[1] Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Cassandra Winfred: \"My assumption is\"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     193.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1763.09 ms /    73 tokens (   24.15 ms per token,    41.40 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1763.32 ms /    74 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[2] Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     193.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    2078.85 ms /    86 tokens (   24.17 ms per token,    41.37 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2078.64 ms /    87 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[3] Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Paige Tyrone: \"I agree with Cassandra's noticing \"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     193.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1945.01 ms /    80 tokens (   24.31 ms per token,    41.13 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1945.38 ms /    81 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[4] Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Paige Tyrone: \"of the author's word choice of barbarism.\"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     193.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    2028.46 ms /    82 tokens (   24.74 ms per token,    40.42 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2028.92 ms /    83 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[5] Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Marissa Roswell: \"I loved the addition of \"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     193.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1831.70 ms /    75 tokens (   24.42 ms per token,    40.95 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1832.13 ms /    76 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[6] Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Marissa Roswell: \"Her lover would die and never love another.\" \"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     193.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1917.79 ms /    79 tokens (   24.28 ms per token,    41.19 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1917.98 ms /    80 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[7] Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     193.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    2208.66 ms /    91 tokens (   24.27 ms per token,    41.20 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2208.76 ms /    92 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[8] Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Paige Tyrone: \"Submitted\"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     193.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1758.94 ms /    72 tokens (   24.43 ms per token,    40.93 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1758.67 ms /    73 tokens\n"
     ]
    }
   ],
   "source": [
    "from itertools import zip_longest\n",
    "GAMMA = 0.999 # Decay factor between [0-1]\n",
    "\n",
    "class Observation:\n",
    "    def __init__(self, time_origin, origin, value, ): # Origin, Emitter, Value\n",
    "        self.time_origin = time_origin\n",
    "        self.recency_value = 1\n",
    "        self.origin = origin # Where it came from\n",
    "        self.value = value # What it contains\n",
    "        self.embed_vector = None\n",
    "        self.get_embed_vector()\n",
    "\n",
    "    def __str__(self,): #{self.origin}\n",
    "        return \"{\"+ f\"[{self.time_origin}] {self.origin}: \\\"{self.value}\\\"\" + \"}\"\n",
    "    \n",
    "    \n",
    "    def get_embed_vector(self,):\n",
    "        if self.embed_vector is None:\n",
    "            print(self.__str__(), \"to embedding...\")\n",
    "            self.embed_vector = get_llama_embedding(self.__str__())\n",
    "        return self.embed_vector\n",
    "        \n",
    "    def decay_recency(self, gamma):\n",
    "        self.recency_value *= gamma\n",
    "\n",
    "observations1 = [Observation(idx, origin, value) for idx, (time_origin, origin, value) in enumerate(observations)]\n",
    "#observations1 = [Observation(idx+1, origin, value) for idx, (origin, value) in enumerate(observations)]\n",
    "\n",
    "# for i in observations1:\n",
    "#     print(i.__str__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_k_obs(query: str, k: int, curr: int):\n",
    "    del observations1[curr]\n",
    "    query_embedding = get_llama_embedding(query)\n",
    "    relevance = [cosine_similarity(query_embedding, obs.get_embed_vector().T) for obs in observations1]\n",
    "    importance = [importance_f(query, obs.__str__()) for obs in observations1]\n",
    "    recency = [obs.recency_value for obs in observations1]\n",
    "\n",
    "    print(relevance)\n",
    "    for idx, (rel, imp, rec) in enumerate(zip(relevance, importance, recency)):\n",
    "        print((idx, rel+imp+rec))\n",
    "  \n",
    "\n",
    "    values = sorted(([(idx, rel+imp+rec) for idx, (rel, imp, rec) in enumerate(zip(relevance, importance, recency))]), key=lambda x: x[1], reverse=True)\n",
    "    print(values)\n",
    "\n",
    "    #rel+imp+rec replace in function \n",
    "    k_best_obs = [observations1[v[0]] for v in values[:k]] \n",
    "\n",
    "    for obs in observations1:\n",
    "        obs.decay_recency(GAMMA)\n",
    "\n",
    "    for obs in k_best_obs:\n",
    "        obs.recency_value = 1\n",
    "\n",
    "    return k_best_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     193.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1773.80 ms /    73 tokens (   24.30 ms per token,    41.15 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1773.94 ms /    74 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rationale: These two messages were sent by the same person but they are unrelated. The second message doesn't provide any additional information related to the query since it's just a greeting."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     219.93 ms\n",
      "llama_print_timings:      sample time =       5.21 ms /    42 runs   (    0.12 ms per token,  8053.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24158.33 ms /   839 tokens (   28.79 ms per token,    34.73 tokens per second)\n",
      "llama_print_timings:        eval time =    5827.30 ms /    41 runs   (  142.13 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   30207.09 ms /   880 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 6\n",
      "Rationale: Both messages contain the same question and pseudonym. But the first message contains the answer to the question making it more related to the query than the unrelated message \"Hello.\" in the second message.\n",
      "I: (importance value)\n",
      "I: 6 In my case, the two messages contain the same question and pseudonym. But the first message contains the answer to the question making it more related to the query than the unrelated message \"Hello.\" in the second message. Hence, the importance value is 6."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     219.93 ms\n",
      "llama_print_timings:      sample time =      14.62 ms /   118 runs   (    0.12 ms per token,  8068.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   16764.29 ms /   118 runs   (  142.07 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   17038.83 ms /   119 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 7\n",
      "Rationale: Both messages are from the same person, but the second message (Query) does not contain any continuation or essential information related to the first message (Observation). However, knowing the first message provides some context to the initial greeting in the second message, making it slightly more important to know the first message to understand the greeting."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     219.93 ms\n",
      "llama_print_timings:      sample time =       9.61 ms /    77 runs   (    0.12 ms per token,  8014.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4932.97 ms /   163 tokens (   30.26 ms per token,    33.04 tokens per second)\n",
      "llama_print_timings:        eval time =   10787.25 ms /    76 runs   (  141.94 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   15926.07 ms /   239 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rationale: These messages are related as they both answer the same question but since one is just a hello and not providing any relevant information to know before answering the query we give it less importance.\n",
      "I: 2\n",
      "\n",
      "O: {[3] Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Paige Tyrone: \"I agree with Cassandra's noticing \"}\n",
      "Q: {[9] Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Cassandra Winfred: \"Hello. I agree with Paige's noticing\"}\n",
      "I: 6.5\n",
      "Rationale: The two messages are related since they agree on answering the same question but only the latter message provides additional information by agreeing with Paige's noticing something relevant to"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     219.93 ms\n",
      "llama_print_timings:      sample time =      31.53 ms /   256 runs   (    0.12 ms per token,  8119.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4585.16 ms /   157 tokens (   29.20 ms per token,    34.24 tokens per second)\n",
      "llama_print_timings:        eval time =   36297.61 ms /   255 runs   (  142.34 ms per token,     7.03 tokens per second)\n",
      "llama_print_timings:       total time =   41550.17 ms /   412 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Please only use the format \"I: (importance value)\" if you want to get points. If you do any other way, you will not get any points.\n",
      "I: 5\n",
      "Rationale: Both messages are related to the same question, but the information in the two messages is not identical nor sequential, therefore not very important to know one to answer the other. This is why I have rated it 5 (somewhere in the middle).<dummy00018> ь ть ь ть ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь ь"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     219.93 ms\n",
      "llama_print_timings:      sample time =      31.54 ms /   256 runs   (    0.12 ms per token,  8116.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4645.15 ms /   159 tokens (   29.21 ms per token,    34.23 tokens per second)\n",
      "llama_print_timings:        eval time =   36318.35 ms /   255 runs   (  142.42 ms per token,     7.02 tokens per second)\n",
      "llama_print_timings:       total time =   41616.75 ms /   414 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rationale: The two queries are related since they ask about the same story's continuation, but since one of them does not actually provide any useful information for answering the question (Cassandra Winfred's \"Hello.\"), the importance of knowing the prior is low.\n",
      "I: 3.5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     219.93 ms\n",
      "llama_print_timings:      sample time =       8.25 ms /    66 runs   (    0.13 ms per token,  7998.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4532.82 ms /   152 tokens (   29.82 ms per token,    33.53 tokens per second)\n",
      "llama_print_timings:        eval time =    9207.80 ms /    65 runs   (  141.66 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:       total time =   13916.88 ms /   217 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rationale: Since the prior (Marissa's response) and the query (Cassandra's response) are both answering the same question and Marissa's prior contains a continuation to what Cassandra's query started (\"Her lover would die and never love another.\"), it's very important to know Marissa's response to understand Cassandra's response properly. The importance rating would be high since knowing Marissa's response would make understanding Cassandra's response much easier and meaningful. Based on the previous examples, I would rate this importance as 9.5, though there could be slight variations depending on how strictly one interprets the definitions and examples.\n",
      "I: 9.5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     219.93 ms\n",
      "llama_print_timings:      sample time =      18.66 ms /   150 runs   (    0.12 ms per token,  8040.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4588.33 ms /   156 tokens (   29.41 ms per token,    34.00 tokens per second)\n",
      "llama_print_timings:        eval time =   21172.69 ms /   149 runs   (  142.10 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   26148.54 ms /   305 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Please only respond in the format \"I: (importance value)\" if you want to get points. If you do any other way, you will not get any points.\n",
      "I: 4.5\n",
      "Rationale: Although both messages contain the same question and pseudonym, they don't continue from each other but instead contain completely different statements that don't necessarily complement each other. Therefore, they are moderately important to know together but not extremely important. This results in a score of 4.5."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     219.93 ms\n",
      "llama_print_timings:      sample time =      13.53 ms /   109 runs   (    0.12 ms per token,  8058.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4873.52 ms /   168 tokens (   29.01 ms per token,    34.47 tokens per second)\n",
      "llama_print_timings:        eval time =   15354.68 ms /   108 runs   (  142.17 ms per token,     7.03 tokens per second)\n",
      "llama_print_timings:       total time =   20516.43 ms /   276 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rationale: In this case, both observations and queries are the same except for the pseudonyms and the words 'Submitted' and 'Hello'. However, these words don't significantly change the meaning or importance of the query given the prior. Thus, knowing the prior (the query) isn't much more important than not knowing it, and so the importance should be low.\n",
      "I: 2\n",
      "Rationale: In this case, both observations and queries are nearly identical except for the pseudonyms and the words 'Submitted' and 'Hello'. However, these words don't significantly change the meaning or importance of the query given the prior. Thus, knowing the prior (the query) isn't much more important than not knowing it, and so the importance should be low.\n",
      "I: 2 Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     219.93 ms\n",
      "llama_print_timings:      sample time =      31.70 ms /   256 runs   (    0.12 ms per token,  8075.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4311.70 ms /   149 tokens (   28.94 ms per token,    34.56 tokens per second)\n",
      "llama_print_timings:        eval time =   36258.05 ms /   255 runs   (  142.19 ms per token,     7.03 tokens per second)\n",
      "llama_print_timings:       total time =   41228.42 ms /   404 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.64754226]]), array([[0.7602322]]), array([[0.68960831]]), array([[0.75315796]]), array([[0.50718729]]), array([[0.72355241]]), array([[0.70556111]]), array([[0.73337518]])]\n",
      "(0, array([[2.24754226]]))\n",
      "(1, array([[2.4602322]]))\n",
      "(2, array([[1.88960831]]))\n",
      "(3, array([[2.25315796]]))\n",
      "(4, array([[1.85718729]]))\n",
      "(5, array([[2.67355241]]))\n",
      "(6, array([[2.15556111]]))\n",
      "(7, array([[1.93337518]]))\n",
      "[(5, array([[2.67355241]])), (1, array([[2.4602322]])), (3, array([[2.25315796]])), (0, array([[2.24754226]])), (6, array([[2.15556111]])), (7, array([[1.93337518]])), (2, array([[1.88960831]])), (4, array([[1.85718729]]))]\n",
      "{[6] Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Marissa Roswell: \"Her lover would die and never love another.\" \"}\n",
      "{[2] Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"}\n",
      "{[4] Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Paige Tyrone: \"of the author's word choice of barbarism.\"}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def memory_module(observations):\n",
    "    for indx, (obs) in enumerate(observations):\n",
    "        k_best_obs = query_k_obs(observations1[indx].__str__(), 3, indx)\n",
    "        for i in k_best_obs:\n",
    "            print(i.__str__())\n",
    "        break\n",
    "    \n",
    "\n",
    "\n",
    "print(memory_module(observations1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.87151981]]\n",
      "[[0.88036487]]\n",
      "[[0.89643193]]\n",
      "[[0.8845694]]\n",
      "[[0.84738461]]\n",
      "[[0.89728706]]\n",
      "[[0.92602138]]\n",
      "[[0.9373653]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     438.75 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1178.09 ms /    25 tokens (   47.12 ms per token,    21.22 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1178.52 ms /    26 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.53871299]]\n"
     ]
    }
   ],
   "source": [
    "for obs in range(len(observations1)-1):\n",
    "    print(cosine_similarity(observations1[obs].get_embed_vector(), observations1[obs+1].get_embed_vector().T))\n",
    "\n",
    "print(cosine_similarity(observations1[obs].get_embed_vector(), get_llama_embedding(\"My teacher always to flunk me till junior high. thanks alot, next semester ill be 35.\").T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ationale: These two messages are from the same person Paige Tyrone and related to the same question but they don't seem to be a continuation or adding more information to each other like in the previous example. Instead they seem to be two separate thoughts or replies to Cassandra Winfred's and Marissa Roswell's answers to the same question. Therefore, while they're related to each other in context they don't share direct sequential connection. Hence, the importance of one to the other is not very high.\n",
      "\n",
      "I: 5\n",
      "Rationale: While both the Observation and Query seem to be related to the same question, they aren't necessarily dependent on each other for understanding or importance. The Observation adds a commentary on the author's word choice while the Query seems to be more focused on seeking agreement or reaction to Cassandra Winfred's answer. They are related but not in a way that the importance of one to the other is very high. Hence, I assign a moderate importance value of 5 to this case."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     215.13 ms\n",
      "llama_print_timings:      sample time =      27.92 ms /   227 runs   (    0.12 ms per token,  8129.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   32266.44 ms /   227 runs   (  142.14 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   32830.36 ms /   228 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "imp = (importance_f(\"{[3] Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Paige Tyrone: \\\"I agree with Cassandra's noticing \\\"}\", \"{[4] Question: Part 1: What happens next? What is behind the door, the lady or the tiger?  Part 2: Later, the King discovers that his daughter has broken the law.  How does the King respond? What happens?, Pseudonym: Paige Tyrone: \\\"of the author's word choice of barbarism.\\\"}\"))\n",
    "\n",
    "print(imp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
