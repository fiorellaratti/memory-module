{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import functions\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import json\n",
    "from llm_demos.llama2.embeddings import llama_embed\n",
    "from llm_demos.llama2.gguf import importance_f, classification\n",
    "from llm_demos.llama2.prompt1 import class_prompt\n",
    "from llm_demos.llama2.prompt2 import uptake_prompt\n",
    "from llm_demos.llama2.prompt3 import question_prompt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama_embedding(document:str):\n",
    "    return np.array(llama_embed([document]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Timestamp('2020-10-20 17:06:00'), 'Cassandra Winfred', 'Hello. '), (Timestamp('2020-10-20 17:06:00'), 'Cassandra Winfred', 'My assumption is'), (Timestamp('2020-10-20 17:06:00'), 'Cassandra Winfred ', 'that the emphasis on barbarism implies that she sent him to the lion.'), (Timestamp('2020-10-27 17:58:00'), 'Paige Tyrone', \"I agree with Cassandra's noticing \"), (Timestamp('2020-10-27 17:58:00'), 'Paige Tyrone', \"of the author's word choice of barbarism.\"), (Timestamp('2020-10-28 20:23:00'), 'Marissa Roswell', 'I loved the addition of '), (Timestamp('2020-10-28 20:23:00'), 'Marissa Roswell', 'Her lover would die and never love another.\" '), (Timestamp('2020-10-28 20:23:00'), 'Marissa Roswell', ' Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"'), (Timestamp('2020-11-11 12:59:00'), 'Paige Tyrone', 'Submitted')]\n"
     ]
    }
   ],
   "source": [
    "## Get data from files\n",
    "\n",
    "# Replace 'your_file_path.xlsm' with the path to your Excel file\n",
    "file_path = '/home/frattitamayo/memory_module/CodingDiscourseAnalysis/CollabWriteAnalysisTest.xlsm'\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(file_path, sheet_name='Test', parse_dates=['Message Time'])\n",
    "observations = []\n",
    "times = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # Construct the observation for each row\n",
    "    time_origin = row['Message Time']\n",
    "    topic = row['Topic']\n",
    "    origin = row['Pseudonym']\n",
    "    # origin = f\"{row['Pseudonym']}\"\n",
    "    value = row['Message']\n",
    "    observation = (time_origin, origin, value)\n",
    "     #observation = [(origin_1, value_1), (origin_2, value_2), ..., (origin_n, value_n)]\n",
    "    observations.append(observation)\n",
    "    \n",
    "print(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[0] Cassandra Winfred: \"Hello. \"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     489.47 ms /    16 tokens (   30.59 ms per token,    32.69 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     489.47 ms /    17 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[1] Cassandra Winfred: \"My assumption is\"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     392.52 ms /    16 tokens (   24.53 ms per token,    40.76 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     392.13 ms /    17 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     708.50 ms /    29 tokens (   24.43 ms per token,    40.93 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     708.78 ms /    30 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[3] Paige Tyrone: \"I agree with Cassandra's noticing \"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     574.13 ms /    23 tokens (   24.96 ms per token,    40.06 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     574.32 ms /    24 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[4] Paige Tyrone: \"of the author's word choice of barbarism.\"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     638.91 ms /    25 tokens (   25.56 ms per token,    39.13 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     639.36 ms /    26 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[5] Marissa Roswell: \"I loved the addition of \"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     458.81 ms /    18 tokens (   25.49 ms per token,    39.23 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     458.56 ms /    19 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[6] Marissa Roswell: \"Her lover would die and never love another.\" \"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     571.69 ms /    22 tokens (   25.99 ms per token,    38.48 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     571.92 ms /    23 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     942.94 ms /    34 tokens (   27.73 ms per token,    36.06 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     943.33 ms /    35 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[8] Paige Tyrone: \"Submitted\"} to embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     442.21 ms /    15 tokens (   29.48 ms per token,    33.92 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     442.50 ms /    16 tokens\n"
     ]
    }
   ],
   "source": [
    "from itertools import zip_longest\n",
    "GAMMA = 0.999 # Decay factor between [0-1]\n",
    "\n",
    "class Observation:\n",
    "    def __init__(self, time_origin, origin, value, ): # Origin, Emitter, Value\n",
    "        self.time_origin = time_origin\n",
    "        self.recency_value = 1\n",
    "        self.origin = origin # Where it came from\n",
    "        self.value = value # What it contains\n",
    "        self.embed_vector = None\n",
    "        self.get_embed_vector()\n",
    "\n",
    "    def __str__(self,): #{self.origin}\n",
    "        return \"{\"+ f\"[{self.time_origin}] {self.origin}: \\\"{self.value}\\\"\" + \"}\"\n",
    "    \n",
    "    \n",
    "    def get_embed_vector(self,):\n",
    "        if self.embed_vector is None:\n",
    "            print(self.__str__(), \"to embedding...\")\n",
    "            self.embed_vector = get_llama_embedding(self.__str__())\n",
    "        return self.embed_vector\n",
    "        \n",
    "    def decay_recency(self, gamma):\n",
    "        self.recency_value *= gamma\n",
    "\n",
    "observations1 = [Observation(idx, origin, value) for idx, (time_origin, origin, value) in enumerate(observations)]\n",
    "#observations1 = [Observation(idx+1, origin, value) for idx, (origin, value) in enumerate(observations)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_k_obs(query: str, k: int, curr: int):\n",
    "    curr_observations = observations1.copy()\n",
    "    del curr_observations[curr]\n",
    "    query_embedding = get_llama_embedding(query)\n",
    "    relevance = [cosine_similarity(query_embedding, obs.get_embed_vector().T) for obs in curr_observations]\n",
    "    importance = [importance_f(query, obs.__str__()) for obs in curr_observations]\n",
    "    recency = [obs.recency_value for obs in curr_observations]\n",
    "\n",
    "    for idx, (rel, imp, rec) in enumerate(zip(relevance, importance, recency)):\n",
    "        print((idx, rel+imp+rec))\n",
    "  \n",
    "\n",
    "    values = sorted(([(idx, rel+imp+rec) for idx, (rel, imp, rec) in enumerate(zip(relevance, importance, recency))]), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    #rel+imp+rec replace in function \n",
    "    k_best_obs = [curr_observations[v[0]] for v in values[:k]] \n",
    "\n",
    "    for obs in curr_observations:\n",
    "        obs.decay_recency(GAMMA)\n",
    "\n",
    "    for obs in k_best_obs:\n",
    "        obs.recency_value = 1\n",
    "\n",
    "    return k_best_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {[0] Cassandra Winfred: \"Hello. \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     443.76 ms /    16 tokens (   27.73 ms per token,    36.06 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     443.93 ms /    17 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[0] Cassandra Winfred: \"Hello. \"} {[1] Cassandra Winfred: \"My assumption is\"}\n",
      "I: 1\n",
      "Rationale: Both observations and query are from the same pseudonym, Cassandra Winfred. However, knowing the prior (\"Hello.\") doesn't provide significant importance for the subsequent statement (\"My assumption is\"), as they seem to be unrelated messages."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.01 ms /    58 runs   (    0.12 ms per token,  8277.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21291.44 ms /   748 tokens (   28.46 ms per token,    35.13 tokens per second)\n",
      "llama_print_timings:        eval time =    8080.23 ms /    57 runs   (  141.76 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   29612.41 ms /   805 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[0] Cassandra Winfred: \"Hello. \"} {[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 1\n",
      "Rationale: The observation contains specific information related to \"the emphasis on barbarism\" and \"the lion,\" while the query is a casual greeting without any connection to the prior observation. Knowing the prior information seems somewhat important but not extremely relevant to the query. Therefore, the importance value lies between 1 to 3, and I'll choose 1 as it's somewhat related but not directly tied to the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      11.33 ms /    95 runs   (    0.12 ms per token,  8383.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1583.86 ms /    55 tokens (   28.80 ms per token,    34.73 tokens per second)\n",
      "llama_print_timings:        eval time =   13452.24 ms /    94 runs   (  143.11 ms per token,     6.99 tokens per second)\n",
      "llama_print_timings:       total time =   15252.22 ms /   149 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[0] Cassandra Winfred: \"Hello. \"} {[3] Paige Tyrone: \"I agree with Cassandra's noticing \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 2\n",
      "Rationale: The observation seems to be responding to something Cassandra Winfred has said or done in the prior message, but since the prior message is just a simple greeting and doesn't contain any specific information or question, the importance of the observation in relation to the greeting is relatively low. Therefore, I assign it a value of 2, which represents a somewhat unimportant relationship between the observation and the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      11.14 ms /    93 runs   (    0.12 ms per token,  8348.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1375.56 ms /    48 tokens (   28.66 ms per token,    34.89 tokens per second)\n",
      "llama_print_timings:        eval time =   13261.89 ms /    93 runs   (  142.60 ms per token,     7.01 tokens per second)\n",
      "llama_print_timings:       total time =   14851.77 ms /   141 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[0] Cassandra Winfred: \"Hello. \"} {[4] Paige Tyrone: \"of the author's word choice of barbarism.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Rationale: The Observation (O) provides insightful analysis on the author's word choice, while the Query (Q) is a simple greeting and not related to the analysis. However, since both messages are from different users, there is some relevance knowing who Paige Tyrone and Cassandra Winfred are in relation to each other, making it a slightly important observation."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      10.06 ms /    84 runs   (    0.12 ms per token,  8349.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1526.10 ms /    51 tokens (   29.92 ms per token,    33.42 tokens per second)\n",
      "llama_print_timings:        eval time =   11773.98 ms /    83 runs   (  141.86 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   13496.23 ms /   134 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 1\n",
      "Rationale: The observation is related to word choice in a piece of writing, while the query is a simple greeting. The importance of knowing the observation for the query is minimal as they are unrelated topics."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       5.91 ms /    49 runs   (    0.12 ms per token,  8298.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    7001.04 ms /    49 runs   (  142.88 ms per token,     7.00 tokens per second)\n",
      "llama_print_timings:       total time =    7112.90 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[0] Cassandra Winfred: \"Hello. \"} {[5] Marissa Roswell: \"I loved the addition of \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Rationale: The Observation O is a part of a story or discussion that seems to be continued from a previous message, while the Query Q is an unrelated, mundane message from another person. Since the query does not depend on the observation to understand its context, the importance of knowing the observation O for the query Q is minimal. Therefore, the importance value is 1."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       9.85 ms /    82 runs   (    0.12 ms per token,  8329.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1308.50 ms /    44 tokens (   29.74 ms per token,    33.63 tokens per second)\n",
      "llama_print_timings:        eval time =   11555.86 ms /    81 runs   (  142.66 ms per token,     7.01 tokens per second)\n",
      "llama_print_timings:       total time =   13057.11 ms /   125 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rationale: These two messages appear to be sent by different people (Marissa Roswell and Cassandra Winfred) and do not seem to be directly related in content. The message from Marissa Roswell seems to be part of a narrative or response to a previous question, while Cassandra Winfred's message is more of a greeting or unrelated statement. As such, knowing the prior (the Marissa Roswell message) is not extremely important for understanding the Cassandra Winfred message."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      12.87 ms /   108 runs   (    0.12 ms per token,  8391.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   15309.56 ms /   108 runs   (  141.76 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   15552.87 ms /   109 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 1\n",
      "Rationale: The two messages are from different people (Marissa Roswell and Cassandra Winfred) and seem unrelated in content. The first message appears to be part of a longer conversation or story, while the second message is simply a greeting. Knowing the prior (the first message) doesn't significantly impact the importance of the query (the second message)."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       9.94 ms /    83 runs   (    0.12 ms per token,  8351.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   11785.66 ms /    83 runs   (  142.00 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   11974.78 ms /    84 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[0] Cassandra Winfred: \"Hello. \"} {[6] Marissa Roswell: \"Her lover would die and never love another.\" \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 1\n",
      "Rationale: The prior (Marissa Roswell's message) is a continuation or completion of the query (Cassandra Winfred's message) in a story or context. Knowing Marissa Roswell's message makes the query significantly more important and understandable. Although not explicitly stated, it seems Marissa's message is a direct response to Cassandra's message which makes it very important in this context."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      11.41 ms /    96 runs   (    0.12 ms per token,  8412.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1428.79 ms /    48 tokens (   29.77 ms per token,    33.59 tokens per second)\n",
      "llama_print_timings:        eval time =   13465.27 ms /    95 runs   (  141.74 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:       total time =   15131.32 ms /   143 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[0] Cassandra Winfred: \"Hello. \"} {[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 1\n",
      "Rationale: Both observations and queries are from different users and seem unrelated. However, since the query is simply a greeting and not asking about any specific information related to the prior observation, the importance of knowing the prior is minimal. Hence, the importance value is 1 (the lowest possible value)."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       8.08 ms /    68 runs   (    0.12 ms per token,  8418.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1762.54 ms /    60 tokens (   29.38 ms per token,    34.04 tokens per second)\n",
      "llama_print_timings:        eval time =    9498.33 ms /    67 runs   (  141.77 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   11431.94 ms /   127 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[0] Cassandra Winfred: \"Hello. \"} {[8] Paige Tyrone: \"Submitted\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 1\n",
      "Rationale: The priority here is knowing what was submitted by Paige Tyrone based on the context given. Cassandra Winfred's message doesn't provide any information that would be necessary to understand or interpret Paige's message better, as it's merely a greeting and unrelated to the submitted content. Therefore, knowing Cassandra's message is not particularly important for understanding or interpreting Paige's query, making the importance value 1."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      12.49 ms /   105 runs   (    0.12 ms per token,  8408.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1155.29 ms /    40 tokens (   28.88 ms per token,    34.62 tokens per second)\n",
      "llama_print_timings:        eval time =   14908.40 ms /   105 runs   (  141.98 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   16315.51 ms /   145 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([[1.82289058]]))\n",
      "(1, array([[1.8716314]]))\n",
      "(2, array([[1.9467831]]))\n",
      "(3, array([[1.87930612]]))\n",
      "(4, array([[1.5669778]]))\n",
      "(5, array([[1.81848715]]))\n",
      "(6, array([[1.82465685]]))\n",
      "(7, array([[1.85329695]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"intent\": \"Social\",\n",
      "    \"rationale\": \"The message Hello. classifies as a social interaction.\"\n",
      "}\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       4.44 ms /    38 runs   (    0.12 ms per token,  8566.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45056.76 ms /  1544 tokens (   29.18 ms per token,    34.27 tokens per second)\n",
      "llama_print_timings:        eval time =    5364.99 ms /    37 runs   (  145.00 ms per token,     6.90 tokens per second)\n",
      "llama_print_timings:       total time =   50768.73 ms /  1581 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {[1] Cassandra Winfred: \"My assumption is\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     403.59 ms /    16 tokens (   25.22 ms per token,    39.64 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     404.10 ms /    17 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[1] Cassandra Winfred: \"My assumption is\"} {[0] Cassandra Winfred: \"Hello. \"}\n",
      "I: 2\n",
      "Rationale: The two messages are both related to the same person (Cassandra Winfred) and are sent sequentially, but the content in the second message seems to be unrelated to the greeting in the first message, making it a low importance observation for this query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.55 ms /    64 runs   (    0.12 ms per token,  8471.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21002.78 ms /   742 tokens (   28.31 ms per token,    35.33 tokens per second)\n",
      "llama_print_timings:        eval time =    8911.88 ms /    63 runs   (  141.46 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:       total time =   30186.08 ms /   805 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[1] Cassandra Winfred: \"My assumption is\"} {[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 2\n",
      "Rationale: The Observation O seems to be unrelated to the Query Q as both statements are from the same person but do not have any direct connection or dependency. The Observation seems to be making a point about a particular context, while the query appears to be asking for an assumption on a different topic or context entirely. Thus, knowing the prior is not very important for the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      10.13 ms /    85 runs   (    0.12 ms per token,  8388.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1586.02 ms /    55 tokens (   28.84 ms per token,    34.68 tokens per second)\n",
      "llama_print_timings:        eval time =   11921.11 ms /    84 runs   (  141.92 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   13713.99 ms /   139 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[1] Cassandra Winfred: \"My assumption is\"} {[3] Paige Tyrone: \"I agree with Cassandra's noticing \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 2\n",
      "Rationale: The Observation O seems to be a response to Cassandra Winfred's query by Paige Tyrone, but it doesn't provide crucial or significant information that would dramatically change the understanding or interpretation of Cassandra's query. As such, the importance of knowing this particular observation in relation to the query is relatively low."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       9.33 ms /    78 runs   (    0.12 ms per token,  8358.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1385.02 ms /    48 tokens (   28.85 ms per token,    34.66 tokens per second)\n",
      "llama_print_timings:        eval time =   11056.37 ms /    78 runs   (  141.75 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   12639.82 ms /   126 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[1] Cassandra Winfred: \"My assumption is\"} {[4] Paige Tyrone: \"of the author's word choice of barbarism.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 1\n",
      "Rationale: The two messages seem unrelated in context; one is discussing word choice and the other is presenting an assumption. Knowing the prior doesn't significantly affect the importance of the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       5.49 ms /    46 runs   (    0.12 ms per token,  8374.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1524.88 ms /    51 tokens (   29.90 ms per token,    33.45 tokens per second)\n",
      "llama_print_timings:        eval time =    6389.62 ms /    45 runs   (  141.99 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =    8033.40 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[1] Cassandra Winfred: \"My assumption is\"} {[5] Marissa Roswell: \"I loved the addition of \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 1\n",
      "Rationale: The two messages seem unrelated, as they were sent by different people (Cassandra Winfred and Marissa Roswell) and the contents of the messages do not seem to build upon each other or rely on knowing the prior information."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       6.91 ms /    58 runs   (    0.12 ms per token,  8391.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1313.92 ms /    44 tokens (   29.86 ms per token,    33.49 tokens per second)\n",
      "llama_print_timings:        eval time =    8091.32 ms /    57 runs   (  141.95 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =    9552.18 ms /   101 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[1] Cassandra Winfred: \"My assumption is\"} {[6] Marissa Roswell: \"Her lover would die and never love another.\" \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 1\n",
      "Rationale: The two observations seem to be unrelated as they are answered by different people (Cassandra Winfred and Marissa Roswell) and the content of the messages have no direct correlation. Therefore, knowing the prior observation is not important for the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.30 ms /    61 runs   (    0.12 ms per token,  8355.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1417.62 ms /    48 tokens (   29.53 ms per token,    33.86 tokens per second)\n",
      "llama_print_timings:        eval time =    8519.09 ms /    60 runs   (  141.98 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   10092.74 ms /   108 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[1] Cassandra Winfred: \"My assumption is\"} {[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 2\n",
      "Rationale: The prior statement by Marissa Roswell seems to be unrelated to the query posed by Cassandra Winfred. It's a personal assumption which doesn't depend on the prior knowledge. Thus, the importance of knowing the prior is low for the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.55 ms /    64 runs   (    0.12 ms per token,  8480.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1831.18 ms /    60 tokens (   30.52 ms per token,    32.77 tokens per second)\n",
      "llama_print_timings:        eval time =    8931.08 ms /    63 runs   (  141.76 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   10920.14 ms /   123 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[1] Cassandra Winfred: \"My assumption is\"} {[8] Paige Tyrone: \"Submitted\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 2\n",
      "Rationale: Both observations and query seem to be unrelated as they are from different people and are not directly connected in context. Thus, knowing the prior (Paige Tyrone's submission) doesn't greatly affect the importance of Cassandra Winfred's assumption. It's more of a coincidence that they are being compared in this task."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       9.61 ms /    82 runs   (    0.12 ms per token,  8537.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1153.76 ms /    40 tokens (   28.84 ms per token,    34.67 tokens per second)\n",
      "llama_print_timings:        eval time =   11604.11 ms /    82 runs   (  141.51 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:       total time =   12955.89 ms /   122 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([[1.92289058]]))\n",
      "(1, array([[1.98664795]]))\n",
      "(2, array([[1.98653061]]))\n",
      "(3, array([[1.89205086]]))\n",
      "(4, array([[1.53463212]]))\n",
      "(5, array([[1.74509705]]))\n",
      "(6, array([[1.91694659]]))\n",
      "(7, array([[1.91717122]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"intent\": \"Deliberation\",\n",
      "    \"rationale\": \"The message \\\"My assumption is\\\" follows the initial discussion about an interpretation of the text (emphasis on barbarism), and as such it can be classified as a deliberation.\"\n",
      "}\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.74 ms /    65 runs   (    0.12 ms per token,  8399.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45149.79 ms /  1536 tokens (   29.39 ms per token,    34.02 tokens per second)\n",
      "llama_print_timings:        eval time =    9275.13 ms /    64 runs   (  144.92 ms per token,     6.90 tokens per second)\n",
      "llama_print_timings:       total time =   54832.56 ms /  1600 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 {[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     723.94 ms /    29 tokens (   24.96 ms per token,    40.06 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     724.31 ms /    30 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"} {[0] Cassandra Winfred: \"Hello. \"}\n",
      "I: 2\n",
      "Rationale: The Observation and Query are both related to Cassandra Winfred's thoughts or messages, but this particular query seems to be only somewhat relevant to the initial greeting. It doesn't provide crucial information that would significantly impact the understanding or importance of the greeting. Therefore, the importance level is moderate but not extremely high."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       9.18 ms /    77 runs   (    0.12 ms per token,  8385.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21691.41 ms /   755 tokens (   28.73 ms per token,    34.81 tokens per second)\n",
      "llama_print_timings:        eval time =   10761.23 ms /    76 runs   (  141.60 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:       total time =   32757.79 ms /   831 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"} {[1] Cassandra Winfred: \"My assumption is\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 7\n",
      "Rationale: Both observations are related to each other and appear to be part of an ongoing discussion or explanation about a specific subject (possibly a literary analysis). Knowing the prior statement helps to understand the context and meaning of the following statement, making it important for the query. The importance level is not as high as in the previous example because these statements seem to be part of an ongoing discussion rather than a direct question and answer exchange."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      11.12 ms /    94 runs   (    0.12 ms per token,  8455.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1589.64 ms /    55 tokens (   28.90 ms per token,    34.60 tokens per second)\n",
      "llama_print_timings:        eval time =   13205.64 ms /    93 runs   (  142.00 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   15021.39 ms /   148 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"} {[3] Paige Tyrone: \"I agree with Cassandra's noticing \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 4.5\n",
      "Rationale: The importance of this observation lies in its relation to Cassandra Winfred's statement, showing agreement and offering additional insight into the interpretation of the emphasis on barbarism. Although not critical for understanding Cassandra's statement alone, it does provide context and additional perspective, making it somewhat valuable to know the prior."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       9.08 ms /    76 runs   (    0.12 ms per token,  8368.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1795.58 ms /    62 tokens (   28.96 ms per token,    34.53 tokens per second)\n",
      "llama_print_timings:        eval time =   10639.62 ms /    75 runs   (  141.86 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   12625.28 ms /   137 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"} {[4] Paige Tyrone: \"of the author's word choice of barbarism.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 5\n",
      "Rationale: Both observations relate to the author's word choice and emphasis on barbarism, but since the queries are asking different things, the importance of knowing the prior is moderately important for understanding the difference in interpretations."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       6.37 ms /    53 runs   (    0.12 ms per token,  8325.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1850.51 ms /    64 tokens (   28.91 ms per token,    34.59 tokens per second)\n",
      "llama_print_timings:        eval time =    7385.90 ms /    52 runs   (  142.04 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =    9372.73 ms /   116 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"} {[5] Marissa Roswell: \"I loved the addition of \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Rationale: Both observations seem to be related to the same question but the first one appears to be a partial answer or continuation to the second one as it ends with \"I loved the addition of\" implying there was an addition to something, possibly the same idea Cassandra Winfred mentioned. Hence knowing the prior is somewhat important for understanding the full context of Cassandra's statement but not as crucial as in the previous example where knowing the prior led to a direct continuation and completion of the same thought."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      12.97 ms /   108 runs   (    0.12 ms per token,  8326.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1608.16 ms /    56 tokens (   28.72 ms per token,    34.82 tokens per second)\n",
      "llama_print_timings:        eval time =   15323.01 ms /   108 runs   (  141.88 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   17201.68 ms /   164 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 4\n",
      "Rationale: Both messages are from different individuals (Marissa Roswell and Cassandra Winfred) and seem to be related to a discussion about an interpretation of a story or text involving a door with either a lady or a tiger behind it. While the messages are somewhat connected in context, knowing the specific content of the prior message isn't extremely important for understanding the query - it's more moderately important."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      10.91 ms /    92 runs   (    0.12 ms per token,  8432.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   13093.19 ms /    92 runs   (  142.32 ms per token,     7.03 tokens per second)\n",
      "llama_print_timings:       total time =   13311.89 ms /    93 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"} {[6] Marissa Roswell: \"Her lover would die and never love another.\" \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 2.5\n",
      "Rationale: The prior (Marissa Roswell's statement) and the query (Cassandra Winfred's statement) seem to be related to the same topic - the implications of a story involving a lady, a tiger, and a king. However, the query does not seem to depend heavily on knowing the prior to understand its meaning or importance, hence a relatively low importance value."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      10.60 ms /    90 runs   (    0.12 ms per token,  8490.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1775.71 ms /    61 tokens (   29.11 ms per token,    34.35 tokens per second)\n",
      "llama_print_timings:        eval time =   12598.72 ms /    89 runs   (  141.56 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:       total time =   14591.54 ms /   150 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"} {[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 4\n",
      "Rationale: The observations and queries are related as they discuss the barbaric nature and actions of the king and princess but the importance of knowing Marissa Roswell's statement isn't extremely high to understand Cassandra Winfred's query as it's not directly connected to her statement. It's somewhat important to understand the context but not crucial. Hence the importance rating of 4."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      10.69 ms /    89 runs   (    0.12 ms per token,  8322.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2193.89 ms /    72 tokens (   30.47 ms per token,    32.82 tokens per second)\n",
      "llama_print_timings:        eval time =   12625.36 ms /    89 runs   (  141.86 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   15050.53 ms /   161 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"} {[8] Paige Tyrone: \"Submitted\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 4\n",
      "Rationale: Both observations and query seem to be related to discussing the same context, which appears to be about analyzing a text or story involving a situation behind a door with either a lady or a tiger. However, the query specifically references a point made by Cassandra Winfred while Paige Tyrone simply states \"Submitted\", making it less important knowing the prior message as it doesn't directly provide further information or clarification on Cassandra Winfred's statement. Hence, the importance value falls in the middle range."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      13.97 ms /   117 runs   (    0.12 ms per token,  8376.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1561.88 ms /    54 tokens (   28.92 ms per token,    34.57 tokens per second)\n",
      "llama_print_timings:        eval time =   16446.06 ms /   116 runs   (  141.78 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   18303.35 ms /   170 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([[1.9716314]]))\n",
      "(1, array([[2.48564795]]))\n",
      "(2, array([[2.26496126]]))\n",
      "(3, array([[2.43900993]]))\n",
      "(4, array([[1.88610903]]))\n",
      "(5, array([[2.03052792]]))\n",
      "(6, array([[2.23188521]]))\n",
      "(7, array([[2.14012579]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"intent\": \"Deliberation\",\n",
      "    \"rationale\": \"The message \"that the emphasis on barbarism implies that she sent him to the lion.\" relates to a deliberation as it discusses interpretation and meaning.\"\n",
      "}\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.19 ms /    61 runs   (    0.12 ms per token,  8485.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45163.67 ms /  1544 tokens (   29.25 ms per token,    34.19 tokens per second)\n",
      "llama_print_timings:        eval time =    8719.95 ms /    60 runs   (  145.33 ms per token,     6.88 tokens per second)\n",
      "llama_print_timings:       total time =   54284.83 ms /  1604 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 {[3] Paige Tyrone: \"I agree with Cassandra's noticing \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     573.67 ms /    23 tokens (   24.94 ms per token,    40.09 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     574.10 ms /    24 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[3] Paige Tyrone: \"I agree with Cassandra's noticing \"} {[0] Cassandra Winfred: \"Hello. \"}\n",
      "I: 2\n",
      "Rationale: Both messages are related as they are responses from different users agreeing with each other's noticing, but since neither message contains specific or important information to the other, their relationship is relatively weak, making the importance of knowing the prior moderate to low."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.21 ms /    60 runs   (    0.12 ms per token,  8320.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21541.47 ms /   749 tokens (   28.76 ms per token,    34.77 tokens per second)\n",
      "llama_print_timings:        eval time =    8364.60 ms /    59 runs   (  141.77 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   30183.20 ms /   808 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[3] Paige Tyrone: \"I agree with Cassandra's noticing \"} {[1] Cassandra Winfred: \"My assumption is\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Rationale: Both messages seem to be related as they appear to be part of a discussion or agreement on a shared idea or notice. However, they aren't very specific to each other or deeply dependent, so their importance isn't very high. It's a common conversation flow but not necessarily crucial to understand the query. Thus, it falls in the middle range around 4."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       9.88 ms /    82 runs   (    0.12 ms per token,  8297.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1399.75 ms /    48 tokens (   29.16 ms per token,    34.29 tokens per second)\n",
      "llama_print_timings:        eval time =   11648.06 ms /    82 runs   (  142.05 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   13260.22 ms /   130 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 3\n",
      "Rationale: Both observations are related as they are referring to the same discussion or event but with different participants expressing their opinions or agreement. The content of the observations isn't highly significant or critical to understand the query, which is agreement with Cassandra's noticing. It's a simple exchange of thoughts or opinions, making the importance moderate."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       9.40 ms /    78 runs   (    0.12 ms per token,  8301.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   11066.50 ms /    78 runs   (  141.88 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   11257.18 ms /    79 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[3] Paige Tyrone: \"I agree with Cassandra's noticing \"} {[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 4\n",
      "Rationale: Both messages are related to the same topic (the emphasis on barbarism and sending someone to the lion), and Paige Tyrone's agreement with Cassandra Winfred's observation implies that understanding Cassandra's prior statement is somewhat important for Paige's response. However, since the agreement doesn't directly depend on every detail of Cassandra's statement, the importance isn't extremely high."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      11.51 ms /    96 runs   (    0.12 ms per token,  8343.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1796.53 ms /    62 tokens (   28.98 ms per token,    34.51 tokens per second)\n",
      "llama_print_timings:        eval time =   13527.16 ms /    95 runs   (  142.39 ms per token,     7.02 tokens per second)\n",
      "llama_print_timings:       total time =   15565.02 ms /   157 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[3] Paige Tyrone: \"I agree with Cassandra's noticing \"} {[4] Paige Tyrone: \"of the author's word choice of barbarism.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 5\n",
      "Rationale: Both observations are related as they are answering the same question and are from the same person (Paige Tyrone). They appear to be part of an ongoing conversation or discussion, making the prior information relevant to the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       6.52 ms /    55 runs   (    0.12 ms per token,  8442.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1769.30 ms /    58 tokens (   30.51 ms per token,    32.78 tokens per second)\n",
      "llama_print_timings:        eval time =    7653.29 ms /    54 runs   (  141.73 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:       total time =    9561.86 ms /   112 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[3] Paige Tyrone: \"I agree with Cassandra's noticing \"} {[5] Marissa Roswell: \"I loved the addition of \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 2\n",
      "Rationale: The two messages seem to be unrelated as they were sent by different people (Marissa Roswell and Paige Tyrone) and mention different contexts (the addition and Cassandra's noticing). The prior doesn't seem to provide significant information for the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.85 ms /    66 runs   (    0.12 ms per token,  8409.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1533.87 ms /    51 tokens (   30.08 ms per token,    33.25 tokens per second)\n",
      "llama_print_timings:        eval time =    9209.02 ms /    65 runs   (  141.68 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:       total time =   10909.02 ms /   116 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[3] Paige Tyrone: \"I agree with Cassandra's noticing \"} {[6] Marissa Roswell: \"Her lover would die and never love another.\" \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 2\n",
      "Rationale: The observation and query seem to be unrelated, as they are from different people discussing separate topics. The importance of knowing the observation is not crucial for understanding or responding to the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       5.46 ms /    46 runs   (    0.12 ms per token,  8420.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1594.61 ms /    55 tokens (   28.99 ms per token,    34.49 tokens per second)\n",
      "llama_print_timings:        eval time =    6384.57 ms /    45 runs   (  141.88 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =    8096.58 ms /   100 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[3] Paige Tyrone: \"I agree with Cassandra's noticing \"} {[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 2\n",
      "Rationale: The Observation O seems to be unrelated to the Query as Paige Tyrone is agreeing with Cassandra's noticing which is not mentioned in Marissa Roswell's statement about the king and princess. Therefore, knowing the prior (Observation) doesn't seem to hold much importance for this particular Query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       9.12 ms /    77 runs   (    0.12 ms per token,  8441.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1989.10 ms /    67 tokens (   29.69 ms per token,    33.68 tokens per second)\n",
      "llama_print_timings:        eval time =   10781.86 ms /    76 runs   (  141.87 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   12963.15 ms /   143 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[3] Paige Tyrone: \"I agree with Cassandra's noticing \"} {[8] Paige Tyrone: \"Submitted\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 4\n",
      "Rationale: The importance lies in the fact that both observations are related to each other as they are responses from Paige Tyrone agreeing with another user (Cassandra Winfred), but they don't hold a very high importance as they are simply agreements or responses to something previously said. It's not a significant revelation or new piece of information that would drastically change the understanding of the context or query at hand. Therefore, it's somewhere in the middle in terms of importance."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      13.08 ms /   110 runs   (    0.12 ms per token,  8407.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1381.32 ms /    48 tokens (   28.78 ms per token,    34.75 tokens per second)\n",
      "llama_print_timings:        eval time =   15435.44 ms /   109 runs   (  141.61 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:       total time =   17088.57 ms /   157 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([[1.9457831]]))\n",
      "(1, array([[2.08653061]]))\n",
      "(2, array([[2.21496126]]))\n",
      "(3, array([[2.34105561]]))\n",
      "(4, array([[1.69709776]]))\n",
      "(5, array([[1.95055555]]))\n",
      "(6, array([[1.94437927]]))\n",
      "(7, array([[2.13064431]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"intent\": \"Deliberation\",\n",
      "    \"rationale\": \"The message \\\"I agree with Cassandra's noticing \\\" is a deliberation because it's discussing the interpretation of content.\"\n",
      "}\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       6.51 ms /    55 runs   (    0.12 ms per token,  8453.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44915.37 ms /  1543 tokens (   29.11 ms per token,    34.35 tokens per second)\n",
      "llama_print_timings:        eval time =    7836.78 ms /    54 runs   (  145.13 ms per token,     6.89 tokens per second)\n",
      "llama_print_timings:       total time =   53146.15 ms /  1597 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 {[4] Paige Tyrone: \"of the author's word choice of barbarism.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     623.08 ms /    25 tokens (   24.92 ms per token,    40.12 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     623.41 ms /    26 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[4] Paige Tyrone: \"of the author's word choice of barbarism.\"} {[0] Cassandra Winfred: \"Hello. \"}\n",
      "I: 2\n",
      "Rationale: The prior (Cassandra Winfred's message) is unrelated to the query (Paige Tyrone's message). It seems like two separate statements without any connection to each other, so the importance level is low."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.02 ms /    59 runs   (    0.12 ms per token,  8400.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21373.79 ms /   751 tokens (   28.46 ms per token,    35.14 tokens per second)\n",
      "llama_print_timings:        eval time =    8202.74 ms /    58 runs   (  141.43 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:       total time =   29843.85 ms /   809 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[4] Paige Tyrone: \"of the author's word choice of barbarism.\"} {[1] Cassandra Winfred: \"My assumption is\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 3\n",
      "Rationale: Both observations are related as they are answering the same question but the messages are not related at all. The prior information is about an assumption made by Cassandra Winfred while the query seeks Paige Tyrone's perspective on the author's word choice of barbarism. The importance of the prior information depends on the individual's emphasis on Cassandra Winfred's assumption, which in this context appears moderate, hence the importance value of 3."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      12.49 ms /   105 runs   (    0.12 ms per token,  8408.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1527.59 ms /    51 tokens (   29.95 ms per token,    33.39 tokens per second)\n",
      "llama_print_timings:        eval time =   14791.90 ms /   104 runs   (  142.23 ms per token,     7.03 tokens per second)\n",
      "llama_print_timings:       total time =   16585.81 ms /   155 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[4] Paige Tyrone: \"of the author's word choice of barbarism.\"} {[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 4\n",
      "Rationale: Both observations focus on the author's use of the word \"barbarism\" in the context of a story or text. While they are not exactly the same, they both revolve around the same concept, making the prior somewhat important for the query. However, since the query specifically asks about the word choice and not the emphasis as in the prior observation, the importance is slightly lower."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      10.38 ms /    88 runs   (    0.12 ms per token,  8475.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1844.70 ms /    64 tokens (   28.82 ms per token,    34.69 tokens per second)\n",
      "llama_print_timings:        eval time =   12344.13 ms /    87 runs   (  141.89 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   14406.46 ms /   151 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[4] Paige Tyrone: \"of the author's word choice of barbarism.\"} {[3] Paige Tyrone: \"I agree with Cassandra's noticing \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 7\n",
      "Rationale: Both observations and query are related as they both mention Paige Tyrone and discuss the author's word choice. The importance of knowing Paige Tyrone's prior notion increases the understanding of the query about the author's word choice."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.16 ms /    60 runs   (    0.12 ms per token,  8375.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1751.76 ms /    58 tokens (   30.20 ms per token,    33.11 tokens per second)\n",
      "llama_print_timings:        eval time =    8408.25 ms /    59 runs   (  142.51 ms per token,     7.02 tokens per second)\n",
      "llama_print_timings:       total time =   10314.76 ms /   117 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[4] Paige Tyrone: \"of the author's word choice of barbarism.\"} {[5] Marissa Roswell: \"I loved the addition of \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 3\n",
      "Rationale: Both messages are related to the same topic (the author's word choice) but they are not directly connected or dependent on each other. They seem to be separate observations about the author's work."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       6.01 ms /    50 runs   (    0.12 ms per token,  8313.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1548.23 ms /    53 tokens (   29.21 ms per token,    34.23 tokens per second)\n",
      "llama_print_timings:        eval time =    6958.83 ms /    49 runs   (  142.02 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =    8639.04 ms /   102 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[4] Paige Tyrone: \"of the author's word choice of barbarism.\"} {[6] Marissa Roswell: \"Her lover would die and never love another.\" \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 2\n",
      "Rationale: The two statements are not directly related as they seem to answer different questions or contexts entirely. The first one appears to be a continuation of a story or thought process from Marissa Roswell while Paige Tyrone's statement seems to be discussing word choice in relation to barbarism which doesn't seem to connect or have relevance to Marissa's statement. Therefore, knowing the prior isn't very important for this query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      12.08 ms /   101 runs   (    0.12 ms per token,  8360.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1610.40 ms /    56 tokens (   28.76 ms per token,    34.77 tokens per second)\n",
      "llama_print_timings:        eval time =   14344.53 ms /   101 runs   (  142.03 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   16213.34 ms /   157 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[4] Paige Tyrone: \"of the author's word choice of barbarism.\"} {[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 4\n",
      "Rationale: The observation and query both mention the author's use of the word \"barbarism\" but do not directly depend on each other to understand the meaning or importance of that concept in their respective messages. It's a slightly related topic but not highly dependent on knowing the prior to understand the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       8.37 ms /    70 runs   (    0.12 ms per token,  8361.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2038.05 ms /    69 tokens (   29.54 ms per token,    33.86 tokens per second)\n",
      "llama_print_timings:        eval time =    9791.16 ms /    69 runs   (  141.90 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   12010.00 ms /   138 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[4] Paige Tyrone: \"of the author's word choice of barbarism.\"} {[8] Paige Tyrone: \"Submitted\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 5\n",
      "Rationale: Both pieces of information are related to Paige Tyrone's submission, but the second one provides additional insight into the author's word choice. Knowing the prior (that Paige Tyrone submitted something) makes the second query somewhat important, but not extremely so, as it only expounds upon the first query's subject."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       9.43 ms /    79 runs   (    0.12 ms per token,  8381.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1529.11 ms /    50 tokens (   30.58 ms per token,    32.70 tokens per second)\n",
      "llama_print_timings:        eval time =   11059.66 ms /    78 runs   (  141.79 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   12794.07 ms /   128 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([[1.97730712]]))\n",
      "(1, array([[2.09105086]]))\n",
      "(2, array([[2.34000993]]))\n",
      "(3, array([[2.54105561]]))\n",
      "(4, array([[1.77829494]]))\n",
      "(5, array([[1.9738969]]))\n",
      "(6, array([[2.24216247]]))\n",
      "(7, array([[2.27699409]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"intent\": \"Seminar\",\n",
      "    \"rationale\": \"The message \\\"of the author's word choice of barbarism.\\\" relates to a seminar intent because it is discussing the meaning or interpretation of content.\"\n",
      "}\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.17 ms /    61 runs   (    0.12 ms per token,  8502.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45140.07 ms /  1543 tokens (   29.25 ms per token,    34.18 tokens per second)\n",
      "llama_print_timings:        eval time =    8702.84 ms /    60 runs   (  145.05 ms per token,     6.89 tokens per second)\n",
      "llama_print_timings:       total time =   54248.25 ms /  1603 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 {[5] Marissa Roswell: \"I loved the addition of \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     461.85 ms /    18 tokens (   25.66 ms per token,    38.97 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     461.69 ms /    19 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[5] Marissa Roswell: \"I loved the addition of \"} {[0] Cassandra Winfred: \"Hello. \"}\n",
      "I: 2\n",
      "Rationale: The Observation O contains a simple greeting while the Query Q mentions a specific addition that seems to be part of a larger context or story. Since the greeting doesn't provide any valuable information towards understanding the query, the importance is considered low in this case."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.63 ms /    64 runs   (    0.12 ms per token,  8387.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21085.13 ms /   744 tokens (   28.34 ms per token,    35.29 tokens per second)\n",
      "llama_print_timings:        eval time =    8941.73 ms /    63 runs   (  141.93 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   30304.63 ms /   807 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[5] Marissa Roswell: \"I loved the addition of \"} {[1] Cassandra Winfred: \"My assumption is\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 2\n",
      "Rationale: The Observation O and Query Q are related as they both contain pseudonyms and seem to be part of a larger conversation or story. However, the information provided in Observation O does not seem to have a significant impact on the content of the Query Q. Therefore, the importance of Observation O for this Query Q is low, resulting in an importance value of 2."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      10.22 ms /    86 runs   (    0.12 ms per token,  8410.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1309.74 ms /    44 tokens (   29.77 ms per token,    33.59 tokens per second)\n",
      "llama_print_timings:        eval time =   12041.67 ms /    85 runs   (  141.67 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:       total time =   13563.62 ms /   129 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[5] Marissa Roswell: \"I loved the addition of \"} {[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 4\n",
      "Rationale: Both observations are related to the same query and seem to be part of the same discussion or story. However, the information in Observation O doesn't appear to be crucial for understanding the importance of the addition mentioned in the Query. Therefore, the importance of the observation given the query is somewhat relevant but not extremely important."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       8.87 ms /    74 runs   (    0.12 ms per token,  8345.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1609.54 ms /    56 tokens (   28.74 ms per token,    34.79 tokens per second)\n",
      "llama_print_timings:        eval time =   10496.54 ms /    74 runs   (  141.85 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   12291.61 ms /   130 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[5] Marissa Roswell: \"I loved the addition of \"} {[3] Paige Tyrone: \"I agree with Cassandra's noticing \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Rationale: Both observations are related to the same query as they are both messages from different users responding to Marissa Roswell's question. However, since they don't provide specific information that would significantly enhance our understanding of Marissa's question, their importance is relatively similar and moderate."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.60 ms /    63 runs   (    0.12 ms per token,  8290.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1525.08 ms /    51 tokens (   29.90 ms per token,    33.44 tokens per second)\n",
      "llama_print_timings:        eval time =    8788.67 ms /    62 runs   (  141.75 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   10468.25 ms /   113 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 3\n",
      "Rationale: Both observations seem to be related to the same conversation or context, but since they only mention other participants in the conversation (Cassandra Winfred and Paige Tyrone), knowing the prior doesn't significantly impact the importance of the query, which is related to the content being discussed. Therefore, the importance of knowing the prior is moderate, but not highly critical to understanding the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      10.51 ms /    89 runs   (    0.12 ms per token,  8470.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   12624.23 ms /    89 runs   (  141.85 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   12837.37 ms /    90 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[5] Marissa Roswell: \"I loved the addition of \"} {[4] Paige Tyrone: \"of the author's word choice of barbarism.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 4.5\n",
      "Rationale: Both observations seem to be related to the same topic, discussing the author's work, and they seem to be continuation of thoughts or responses by different people (Paige Tyrone and Marissa Roswell). The importance lies in understanding the context of the conversation around the author's work and word choice. Thus, knowing the prior information about Paige Tyrone's statement makes Marissa Roswell's statement more meaningful in this context."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      12.52 ms /   105 runs   (    0.12 ms per token,  8386.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1546.03 ms /    53 tokens (   29.17 ms per token,    34.28 tokens per second)\n",
      "llama_print_timings:        eval time =   14765.66 ms /   104 runs   (  141.98 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   16575.15 ms /   157 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[5] Marissa Roswell: \"I loved the addition of \"} {[6] Marissa Roswell: \"Her lover would die and never love another.\" \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 9.5\n",
      "Rationale: The two messages appear to be sent in order by the same person, and the second message \"I loved the addition of \" seems to be a continuation of the first message \"Her lover would die and never love another.\" Therefore, knowing the prior (first message) is quite important for understanding the second message and the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       9.21 ms /    77 runs   (    0.12 ms per token,  8360.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1529.38 ms /    50 tokens (   30.59 ms per token,    32.69 tokens per second)\n",
      "llama_print_timings:        eval time =   10805.73 ms /    76 runs   (  142.18 ms per token,     7.03 tokens per second)\n",
      "llama_print_timings:       total time =   12528.01 ms /   126 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[5] Marissa Roswell: \"I loved the addition of \"} {[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 6\n",
      "Rationale: Both observations seem to be related to the same topic as they both contain text from Marissa Roswell. Although they aren't consecutive in context like the previous example, they both contribute to understanding the overall theme or narrative being presented by Marissa Roswell; however, since they aren't as directly connected as the previous example, the importance value falls slightly in between."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      10.12 ms /    84 runs   (    0.12 ms per token,  8301.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1807.88 ms /    62 tokens (   29.16 ms per token,    34.29 tokens per second)\n",
      "llama_print_timings:        eval time =   11773.01 ms /    83 runs   (  141.84 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   13792.70 ms /   145 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[5] Marissa Roswell: \"I loved the addition of \"} {[8] Paige Tyrone: \"Submitted\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Rationale: The prior information, \"Paige Tyrone: 'Submitted'\", does not have any direct relevance to the query, which is a continuation of \"Marissa Roswell: 'I loved the addition of'\". The query does not depend on the prior information to make sense or to understand its context, making the importance of the prior relatively low in this case."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       9.98 ms /    83 runs   (    0.12 ms per token,  8312.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1297.32 ms /    43 tokens (   30.17 ms per token,    33.15 tokens per second)\n",
      "llama_print_timings:        eval time =   11636.84 ms /    82 runs   (  141.91 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   13137.24 ms /   125 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 2\n",
      "Rationale: Both observations and query seem to be unrelated as they come from different people (Paige Tyrone and Marissa Roswell) and different messages. Thus, knowing the prior (Paige's message) does not seem to be very important for the query (Marissa's message)."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       8.53 ms /    71 runs   (    0.12 ms per token,  8323.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   10075.69 ms /    71 runs   (  141.91 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   10247.27 ms /    72 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([[1.6639808]]))\n",
      "(1, array([[1.63363312]]))\n",
      "(2, array([[1.88810803]]))\n",
      "(3, array([[1.80009476]]))\n",
      "(4, array([[1.93228895]]))\n",
      "(5, array([[2.55656149]]))\n",
      "(6, array([[2.09388599]]))\n",
      "(7, array([[1.66016174]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"intent\": \"UX\",\n",
      "    \"rationale\": \"The message 'I loved the addition of \" ' expresses the user's opinion about the content or media which relates to UX intent.\"\n",
      "}\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       6.60 ms /    56 runs   (    0.12 ms per token,  8484.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45129.48 ms /  1550 tokens (   29.12 ms per token,    34.35 tokens per second)\n",
      "llama_print_timings:        eval time =    7985.95 ms /    55 runs   (  145.20 ms per token,     6.89 tokens per second)\n",
      "llama_print_timings:       total time =   53511.78 ms /  1605 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 {[6] Marissa Roswell: \"Her lover would die and never love another.\" \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     552.22 ms /    22 tokens (   25.10 ms per token,    39.84 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     552.21 ms /    23 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[6] Marissa Roswell: \"Her lover would die and never love another.\" \"} {[0] Cassandra Winfred: \"Hello. \"}\n",
      "I: 2\n",
      "Rationale: The two messages seem to be unrelated as they have different pseudonyms and the content of the messages does not indicate any direct connection between them. Thus, knowing the prior (the first message) is not very important for the query (the second message)."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.23 ms /    61 runs   (    0.12 ms per token,  8438.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21230.78 ms /   748 tokens (   28.38 ms per token,    35.23 tokens per second)\n",
      "llama_print_timings:        eval time =    8518.22 ms /    60 runs   (  141.97 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   30015.54 ms /   808 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[6] Marissa Roswell: \"Her lover would die and never love another.\" \"} {[1] Cassandra Winfred: \"My assumption is\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 9.5\n",
      "Rationale: These two messages seem to be related in terms of content and context, as they appear to be part of a narrative or story being told by two different people (Cassandra Winfred and Marissa Roswell). Knowing the prior message \"My assumption is\" makes understanding the latter message \"Her lover would die and never love another.\" more meaningful and important for the query. Thus, the importance rating is 9.5."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      11.92 ms /    99 runs   (    0.12 ms per token,  8306.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1380.06 ms /    48 tokens (   28.75 ms per token,    34.78 tokens per second)\n",
      "llama_print_timings:        eval time =   13926.47 ms /    98 runs   (  142.11 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   15552.18 ms /   146 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[6] Marissa Roswell: \"Her lover would die and never love another.\" \"} {[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 9.5\n",
      "Rationale: Similar to the previous example, these two messages seem to be sent in order by the same person (Cassandra Winfred and Marissa Roswell). The prior message talks about the emphasis on barbarism and sending someone to the lion, which sets up the context for the following message from Marissa Roswell about \"Her lover would die and never love another.\" In this case, knowing the prior is important for understanding the full context and conclusion in the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      12.68 ms /   106 runs   (    0.12 ms per token,  8358.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1782.38 ms /    61 tokens (   29.22 ms per token,    34.22 tokens per second)\n",
      "llama_print_timings:        eval time =   14934.37 ms /   105 runs   (  142.23 ms per token,     7.03 tokens per second)\n",
      "llama_print_timings:       total time =   16983.09 ms /   166 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[6] Marissa Roswell: \"Her lover would die and never love another.\" \"} {[3] Paige Tyrone: \"I agree with Cassandra's noticing \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Rationale: Both O and Q seem to be related to the same question or story as they both include parts of it, but they don't directly depend on each other to form a coherent answer. They provide separate pieces of information from different participants (Cassandra Winfred and Marissa Roswell), making the importance of the prior moderate in relation to the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       9.59 ms /    80 runs   (    0.12 ms per token,  8345.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1600.69 ms /    55 tokens (   29.10 ms per token,    34.36 tokens per second)\n",
      "llama_print_timings:        eval time =   11251.24 ms /    79 runs   (  142.42 ms per token,     7.02 tokens per second)\n",
      "llama_print_timings:       total time =   13050.66 ms /   134 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 5\n",
      "Rationale: The observations seem to be related to the same query but they don't provide critical information about it. They are more like comments or replies rather than essential pieces of information. Thus, they have medium importance."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       6.20 ms /    52 runs   (    0.12 ms per token,  8391.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    7351.17 ms /    52 runs   (  141.37 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:       total time =    7470.00 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[6] Marissa Roswell: \"Her lover would die and never love another.\" \"} {[4] Paige Tyrone: \"of the author's word choice of barbarism.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 8.5\n",
      "Rationale: Both statements seem to be related to the same context, as they appear to be responses to a question about a story or text. The specific content of each statement suggests they could be part of a larger discussion or narrative, making the prior important for understanding the query. However, since we don't have direct information about the query being about the same subject or question, the importance is not at the highest level."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      11.17 ms /    94 runs   (    0.12 ms per token,  8417.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1607.88 ms /    56 tokens (   28.71 ms per token,    34.83 tokens per second)\n",
      "llama_print_timings:        eval time =   13322.63 ms /    94 runs   (  141.73 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:       total time =   15172.23 ms /   150 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[6] Marissa Roswell: \"Her lover would die and never love another.\" \"} {[5] Marissa Roswell: \"I loved the addition of \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.5\n",
      "Rationale: Similar to the previous example, these two messages were sent in order by the same person and the second message is a continuation of the first one. Knowing the prior \"I loved the addition of\" makes understanding the query \"Her lover would die and never love another.\" much more important."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.84 ms /    67 runs   (    0.12 ms per token,  8540.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1645.60 ms /    50 tokens (   32.91 ms per token,    30.38 tokens per second)\n",
      "llama_print_timings:        eval time =    9400.94 ms /    66 runs   (  142.44 ms per token,     7.02 tokens per second)\n",
      "llama_print_timings:       total time =   11217.09 ms /   116 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       0.48 ms /     4 runs   (    0.12 ms per token,  8421.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     577.44 ms /     4 runs   (  144.36 ms per token,     6.93 tokens per second)\n",
      "llama_print_timings:       total time =     586.71 ms /     5 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".5\n",
      "Rationale: By the index and the Pseudonym you can assume this two messages were sent in order by the same person and therefore \"Her lover would die and never love another.\" its a continuation of \"I loved the addition of \" which makes knowing the prior really important for the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.78 ms /    66 runs   (    0.12 ms per token,  8480.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    9373.44 ms /    66 runs   (  142.02 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =    9532.64 ms /    67 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".5\n",
      "Rationale: Similar to the previous example, these two messages were sent in order by the same person and the second message is a continuation of the first message. Therefore, knowing the prior \"I loved the addition of \" is really important for understanding the query \"Her lover would die and never love another.\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.99 ms /    67 runs   (    0.12 ms per token,  8385.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    9488.30 ms /    67 runs   (  141.62 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:       total time =    9656.57 ms /    68 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       0.48 ms /     4 runs   (    0.12 ms per token,  8368.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     568.61 ms /     4 runs   (  142.15 ms per token,     7.03 tokens per second)\n",
      "llama_print_timings:       total time =     578.27 ms /     5 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       0.47 ms /     4 runs   (    0.12 ms per token,  8438.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     567.36 ms /     4 runs   (  141.84 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =     576.57 ms /     5 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".5\n",
      "Rationale: Similar to the previous example, these two messages were sent in order by the same person and therefore \"Her lover would die and never love another.\" is a continuation of \"I loved the addition of \". The prior information is quite important for the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       6.91 ms /    59 runs   (    0.12 ms per token,  8543.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    8364.45 ms /    59 runs   (  141.77 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =    8503.15 ms /    60 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".5\n",
      "Rationale: The prior statement \"I loved the addition of \" sets up the context for the latter statement \"Her lover would die and never love another.\" Knowing the prior is essential to understand the full narrative being presented in the query. Therefore, the importance of knowing the prior is quite high, resulting in an importance value of 9.5."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       8.94 ms /    76 runs   (    0.12 ms per token,  8502.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   10763.38 ms /    76 runs   (  141.62 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:       total time =   10943.32 ms /    77 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".5\n",
      "Rationale: As explained in the example, by the index and the Pseudonym you can assume that these two messages were sent in order by the same person, and therefore \"Her lover would die and never love another.\" is a continuation of \"I loved the addition of \". This makes knowing the prior really important for the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       8.71 ms /    74 runs   (    0.12 ms per token,  8492.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   10485.83 ms /    74 runs   (  141.70 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:       total time =   10661.06 ms /    75 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       0.47 ms /     4 runs   (    0.12 ms per token,  8565.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     565.96 ms /     4 runs   (  141.49 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:       total time =     575.74 ms /     5 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".5\n",
      "Rationale: The prior message \"I loved the addition of \" sets up context for the response \"Her lover would die and never love another.\" Since these messages are related and part of a continuous thought or story, knowing the prior is quite important for understanding the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       6.93 ms /    59 runs   (    0.12 ms per token,  8512.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    8368.15 ms /    59 runs   (  141.83 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =    8508.38 ms /    60 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 9.5\n",
      "Rationale: Similar to the previous example, these two messages appear to be sent in order by the same person, with the second message being a continuation of the first. Therefore, knowing the prior (\"I loved the addition of \") is very important for understanding the query (\"Her lover would die and never love another.\")."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       8.73 ms /    73 runs   (    0.12 ms per token,  8361.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   10378.13 ms /    73 runs   (  142.17 ms per token,     7.03 tokens per second)\n",
      "llama_print_timings:       total time =   10561.28 ms /    74 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[6] Marissa Roswell: \"Her lover would die and never love another.\" \"} {[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 9.5\n",
      "Rationale: Similar to the previous example, these two messages seem to be sent in order by the same person (Marissa Roswell) and are related to each other. The second message appears to be a continuation of the first one, discussing the consequences of the king and princess' actions in the context of crimes of passion. Therefore, knowing the prior (the message about the barbaric nature) is quite important for understanding the query (the lover's fate)."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      12.38 ms /   104 runs   (    0.12 ms per token,  8404.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2046.81 ms /    66 tokens (   31.01 ms per token,    32.25 tokens per second)\n",
      "llama_print_timings:        eval time =   14612.95 ms /   103 runs   (  141.87 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   16921.47 ms /   169 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[6] Marissa Roswell: \"Her lover would die and never love another.\" \"} {[8] Paige Tyrone: \"Submitted\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 6\n",
      "\n",
      "Rationale: The Observation O contains Paige Tyrone's submission while the Query contains Marissa Roswell's response. Although they seem to be related in terms of content, since they were not submitted together and are not consecutive like our previous example, their connection isn't as strong. Therefore, the importance of knowing the prior observation isn't exceptionally high for this specific query, but it's still somewhat relevant. Hence, the importance value is 6."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      12.47 ms /   106 runs   (    0.12 ms per token,  8499.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1356.25 ms /    47 tokens (   28.86 ms per token,    34.65 tokens per second)\n",
      "llama_print_timings:        eval time =   14884.72 ms /   105 runs   (  141.76 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   16510.35 ms /   152 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([[1.91449315]]))\n",
      "(1, array([[2.59310005]]))\n",
      "(2, array([[2.73152692]]))\n",
      "(3, array([[2.25255256]]))\n",
      "(4, array([[2.62789091]]))\n",
      "(5, array([[2.55656149]]))\n",
      "(6, array([[2.72885413]]))\n",
      "(7, array([[2.30978301]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"intent\": \"Seminar\",\n",
      "    \"rationale\": \"The message 'Her lover would die and never love another.' is an interpretation and discussion of the content.\"\n",
      "}\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       5.74 ms /    48 runs   (    0.12 ms per token,  8362.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45633.06 ms /  1560 tokens (   29.25 ms per token,    34.19 tokens per second)\n",
      "llama_print_timings:        eval time =    6993.67 ms /    48 runs   (  145.70 ms per token,     6.86 tokens per second)\n",
      "llama_print_timings:       total time =   53006.76 ms /  1608 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 {[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     925.01 ms /    34 tokens (   27.21 ms per token,    36.76 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     924.95 ms /    35 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"} {[0] Cassandra Winfred: \"Hello. \"}\n",
      "I: 2\n",
      "Rationale: The prior (Cassandra Winfred's message) seems to be an unrelated and mundane greeting, while the query (Marissa Roswell's message) discusses a deeper topic related to the barbaric nature of royalty and its connection to crimes of passion. Knowing the prior is not significantly important for understanding or relating to the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      10.10 ms /    86 runs   (    0.12 ms per token,  8519.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21612.94 ms /   760 tokens (   28.44 ms per token,    35.16 tokens per second)\n",
      "llama_print_timings:        eval time =   12040.64 ms /    85 runs   (  141.65 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:       total time =   33997.56 ms /   845 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"} {[1] Cassandra Winfred: \"My assumption is\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Rationale: Both messages seem to be related to the topic of the king and princess, but they don't directly continue or answer each other. They express different opinions or observations about the same subject, making the prior only somewhat relevant for the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       6.39 ms /    54 runs   (    0.12 ms per token,  8452.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1868.36 ms /    60 tokens (   31.14 ms per token,    32.11 tokens per second)\n",
      "llama_print_timings:        eval time =    7499.93 ms /    53 runs   (  141.51 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:       total time =    9511.30 ms /   113 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 4\n",
      "Rationale: The two messages seem to be related in that they both mention \"Cassandra Winfred\" and \"Marissa Roswell\" and appear to be part of an ongoing discussion or debate, but the specific content of each message is not directly connected. As such, knowing the prior isn't highly critical to understanding the query. It's more of a conversation context than direct related information."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      10.51 ms /    89 runs   (    0.12 ms per token,  8465.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   12609.00 ms /    89 runs   (  141.67 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:       total time =   12827.48 ms /    90 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"} {[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 7\n",
      "Rationale: Both observations are related to the theme of barbarism and are connected to each other as they discuss the implications of such actions in a historical context and its relation to modern-day crimes of passion. The importance lies in understanding the context and connection between the two statements."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.58 ms /    63 runs   (    0.12 ms per token,  8315.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2076.36 ms /    72 tokens (   28.84 ms per token,    34.68 tokens per second)\n",
      "llama_print_timings:        eval time =    8964.30 ms /    63 runs   (  142.29 ms per token,     7.03 tokens per second)\n",
      "llama_print_timings:       total time =   11202.50 ms /   135 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"} {[3] Paige Tyrone: \"I agree with Cassandra's noticing \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n",
      "\n",
      "Rationale: Both observations are related to the same query in terms of discussing the nature of the king and princess. However, they do not seem to be directly dependent on each other as they express different opinions, making the importance of knowing the prior not too significant for the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.33 ms /    62 runs   (    0.12 ms per token,  8457.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2000.45 ms /    67 tokens (   29.86 ms per token,    33.49 tokens per second)\n",
      "llama_print_timings:        eval time =    8675.51 ms /    61 runs   (  142.22 ms per token,     7.03 tokens per second)\n",
      "llama_print_timings:       total time =   10834.43 ms /   128 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 5\n",
      "Rationale: Both observations and query seem to be related to the discussions or stories that involve characters like a king, princess, or their actions. However, they don't directly depend on each other, making the importance moderate. It's not a continuation or direct answer to a prior question but rather seems to be discussing similar themes or contexts."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       9.19 ms /    78 runs   (    0.12 ms per token,  8489.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   11085.88 ms /    78 runs   (  142.13 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   11280.27 ms /    79 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"} {[4] Paige Tyrone: \"of the author's word choice of barbarism.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 8\n",
      "Rationale: Both observations are related to the topic of \"barbarism\" and seem to be part of a conversation or discussion on the subject. They are not unrelated mundane tasks or unrelated messages but rather directly connected to each other in terms of content. The importance lies in understanding the context of the discussion on barbarism and its connection to the actions of the king and princess mentioned in the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      10.69 ms /    91 runs   (    0.12 ms per token,  8516.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2008.79 ms /    69 tokens (   29.11 ms per token,    34.35 tokens per second)\n",
      "llama_print_timings:        eval time =   12742.55 ms /    90 runs   (  141.58 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:       total time =   14988.07 ms /   159 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"} {[5] Marissa Roswell: \"I loved the addition of \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 9.5\n",
      "Rationale: The two messages appear to be related in context and seem to be part of a continuous thought or narrative being shared by the user Marissa Roswell. The importance of knowing the prior statement \"I loved the addition of \" helps in understanding the context of the second message and its connection to the topic discussed in the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       8.84 ms /    75 runs   (    0.12 ms per token,  8486.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1799.32 ms /    62 tokens (   29.02 ms per token,    34.46 tokens per second)\n",
      "llama_print_timings:        eval time =   10468.84 ms /    74 runs   (  141.47 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:       total time =   12457.85 ms /   136 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"} {[6] Marissa Roswell: \"Her lover would die and never love another.\" \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 9\n",
      "Rationale: Both observations are related to each other as they both come from the same pseudonym Marissa Roswell and seem to be part of the same narrative or thought process. The second observation builds upon the first one by making a connection to modern societal issues, which makes knowing the prior important for understanding the query. While it's not directly asked for in the query, knowing the prior statement about her lover dying adds depth to the interpretation of the second statement, making it more important. Hence, the importance value of 9."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      13.54 ms /   115 runs   (    0.12 ms per token,  8492.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1986.90 ms /    66 tokens (   30.10 ms per token,    33.22 tokens per second)\n",
      "llama_print_timings:        eval time =   16564.52 ms /   114 runs   (  145.30 ms per token,     6.88 tokens per second)\n",
      "llama_print_timings:       total time =   18844.75 ms /   180 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"} {[8] Paige Tyrone: \"Submitted\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 5\n",
      "Rationale: The two pieces of information are related in that they both contain responses from different people, but they do not directly depend on each other or reveal any progression or correlation in content. The connection between them seems weak which makes knowing the prior not very important for this query. Hence, the importance value lies in the middle range (5 out of 10)."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       9.71 ms /    81 runs   (    0.12 ms per token,  8342.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2251.22 ms /    59 tokens (   38.16 ms per token,    26.21 tokens per second)\n",
      "llama_print_timings:        eval time =   11392.71 ms /    80 runs   (  142.41 ms per token,     7.02 tokens per second)\n",
      "llama_print_timings:       total time =   13847.16 ms /   139 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([[1.91966684]]))\n",
      "(1, array([[2.11395259]]))\n",
      "(2, array([[2.53388421]]))\n",
      "(3, array([[2.24537727]]))\n",
      "(4, array([[2.64615648]]))\n",
      "(5, array([[2.44289098]]))\n",
      "(6, array([[2.67885413]]))\n",
      "(7, array([[2.22398825]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"intent\": \"Seminar\",\n",
      "    \"rationale\": \"The message \\\"Sadly the barbaric nature of the king and princess is still alive today in crimes of passion\\\" reflects an interpretation and comparison to modern times, which fits into the Seminar intent.\"\n",
      "}\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       8.20 ms /    69 runs   (    0.12 ms per token,  8412.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =   63742.16 ms /  1562 tokens (   40.81 ms per token,    24.50 tokens per second)\n",
      "llama_print_timings:        eval time =    9893.45 ms /    68 runs   (  145.49 ms per token,     6.87 tokens per second)\n",
      "llama_print_timings:       total time =   74083.83 ms /  1630 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 {[8] Paige Tyrone: \"Submitted\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     158.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     384.58 ms /    15 tokens (   25.64 ms per token,    39.00 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     384.82 ms /    16 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[8] Paige Tyrone: \"Submitted\"} {[0] Cassandra Winfred: \"Hello. \"}\n",
      "I: 3\n",
      "Rationale: Both messages are related as they are answers to the same question but the content in each message is not closely related to each other. It's more about the action of submitting the answer rather than the actual content of the answer itself."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       6.87 ms /    57 runs   (    0.12 ms per token,  8293.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21136.85 ms /   741 tokens (   28.52 ms per token,    35.06 tokens per second)\n",
      "llama_print_timings:        eval time =    8024.54 ms /    56 runs   (  143.30 ms per token,     6.98 tokens per second)\n",
      "llama_print_timings:       total time =   29425.65 ms /   797 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[8] Paige Tyrone: \"Submitted\"} {[1] Cassandra Winfred: \"My assumption is\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 2\n",
      "Rationale: Both messages are related as they are responses from different people (Cassandra Winfred and Paige Tyrone) to the same query, but the content of the messages is not directly connected or dependent on each other, making the importance of knowing the prior relatively low for the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       8.14 ms /    68 runs   (    0.12 ms per token,  8352.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1452.74 ms /    40 tokens (   36.32 ms per token,    27.53 tokens per second)\n",
      "llama_print_timings:        eval time =    9990.85 ms /    68 runs   (  146.92 ms per token,     6.81 tokens per second)\n",
      "llama_print_timings:       total time =   11614.24 ms /   108 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[8] Paige Tyrone: \"Submitted\"} {[2] Cassandra Winfred : \"that the emphasis on barbarism implies that she sent him to the lion.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 3\n",
      "Rationale: Both observations and queries are related as they seem to be answering the same question, but the content of the messages is not directly connected. The emphasis on barbarism mentioned by Cassandra Winfred doesn't provide crucial information for understanding Paige Tyrone's \"Submitted\" message. Therefore, the importance of knowing the prior is moderate."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       9.65 ms /    81 runs   (    0.12 ms per token,  8389.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1736.23 ms /    54 tokens (   32.15 ms per token,    31.10 tokens per second)\n",
      "llama_print_timings:        eval time =   11434.71 ms /    80 runs   (  142.93 ms per token,     7.00 tokens per second)\n",
      "llama_print_timings:       total time =   13373.64 ms /   134 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[8] Paige Tyrone: \"Submitted\"} {[3] Paige Tyrone: \"I agree with Cassandra's noticing \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Rationale: The observation (O) contains Paige Tyrone's agreement with Cassandra's noticing, but the query (Q) is about Paige Tyrone submitting something, which is not directly related to the agreement mentioned in O. Therefore, the importance of knowing the prior for the query is relatively low."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       8.55 ms /    72 runs   (    0.12 ms per token,  8418.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1430.51 ms /    48 tokens (   29.80 ms per token,    33.55 tokens per second)\n",
      "llama_print_timings:        eval time =   10107.66 ms /    71 runs   (  142.36 ms per token,     7.02 tokens per second)\n",
      "llama_print_timings:       total time =   11721.01 ms /   119 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".5\n",
      "Rationale: Both observations and query are related to Paige Tyrone's name, but the content doesn't reveal a strong connection between them. The first message seems to be agreeing with something noticed by Cassandra Winfred (from a previous example), while the second message indicates that Paige Tyrone has submitted something. The importance of knowing the prior is moderate but not very high since we don't get much information about the context or content of Paige Tyrone's submission."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      13.20 ms /   110 runs   (    0.12 ms per token,  8333.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   15614.28 ms /   110 runs   (  141.95 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   15895.35 ms /   111 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       0.24 ms /     2 runs   (    0.12 ms per token,  8264.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     283.75 ms /     2 runs   (  141.87 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =     288.71 ms /     3 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rationale: Both the observation and query are related to Paige Tyrone's submission but they don't provide any specific information that would make knowing the prior exceptionally important for the query. It's more about agreeing on something that was already submitted rather than providing new or crucial information."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       7.73 ms /    65 runs   (    0.12 ms per token,  8408.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    9225.24 ms /    65 runs   (  141.93 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =    9387.49 ms /    66 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 3\n",
      "Rationale: Both messages are from Paige Tyrone and seem to be related to a discussion or shared information, but they don't necessarily provide significant details about each other. It's not clear if agreeing with a notion is directly connected or influential to submitting something, so the importance of knowing the prior isn't very high for the query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       9.44 ms /    80 runs   (    0.12 ms per token,  8476.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   11356.86 ms /    80 runs   (  141.96 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   11552.36 ms /    81 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[8] Paige Tyrone: \"Submitted\"} {[4] Paige Tyrone: \"of the author's word choice of barbarism.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 5\n",
      "Rationale: Both messages are related as they are both from Paige Tyrone and likely part of the same discussion or assignment. However, since the first message contains specific analysis on the author's word choice and the second message simply indicates submission, the importance of knowing the first message isn't extremely high for the second message. Therefore, the importance value falls in the middle range."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      10.09 ms /    85 runs   (    0.12 ms per token,  8428.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1526.53 ms /    50 tokens (   30.53 ms per token,    32.75 tokens per second)\n",
      "llama_print_timings:        eval time =   11893.53 ms /    84 runs   (  141.59 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:       total time =   13631.51 ms /   134 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[8] Paige Tyrone: \"Submitted\"} {[5] Marissa Roswell: \"I loved the addition of \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 3\n",
      "Rationale: Both messages are related to the same question and pseudonym, but they don't provide specific information about each other. They are related in context but not in content, so the importance of knowing the prior is moderate."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       6.28 ms /    53 runs   (    0.12 ms per token,  8438.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1442.96 ms /    43 tokens (   33.56 ms per token,    29.80 tokens per second)\n",
      "llama_print_timings:        eval time =    7334.37 ms /    52 runs   (  141.05 ms per token,     7.09 tokens per second)\n",
      "llama_print_timings:       total time =    8909.01 ms /    95 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[8] Paige Tyrone: \"Submitted\"} {[6] Marissa Roswell: \"Her lover would die and never love another.\" \"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 3\n",
      "Rationale: Both of these observations are related to the same query since they seem to be answers to the same question provided in the example, but they do not share any direct connection or provide new information about each other. They simply represent different people answering the same question. Therefore, knowing the prior (Marissa Roswell's response) doesn't significantly impact understanding or evaluating Paige Tyrone's \"Submitted\" response."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =      11.42 ms /    96 runs   (    0.12 ms per token,  8409.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1358.42 ms /    47 tokens (   28.90 ms per token,    34.60 tokens per second)\n",
      "llama_print_timings:        eval time =   13433.35 ms /    95 runs   (  141.40 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:       total time =   15033.89 ms /   142 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[8] Paige Tyrone: \"Submitted\"} {[7] Marissa Roswell: \" Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 2\n",
      "Rationale: The prior statement is about the barbaric nature of the king and princess in a story, while the query is about something being submitted, likely unrelated to the prior statement. The importance of knowing the prior is low for this query."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       6.86 ms /    57 runs   (    0.12 ms per token,  8305.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1758.47 ms /    59 tokens (   29.80 ms per token,    33.55 tokens per second)\n",
      "llama_print_timings:        eval time =    7965.04 ms /    56 runs   (  142.23 ms per token,     7.03 tokens per second)\n",
      "llama_print_timings:       total time =    9877.56 ms /   115 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([[2.04731193]]))\n",
      "(1, array([[1.91318121]]))\n",
      "(2, array([[2.04212479]]))\n",
      "(3, array([[2.03064431]]))\n",
      "(4, array([[2.27699409]]))\n",
      "(5, array([[1.7531827]]))\n",
      "(6, array([[2.01078301]]))\n",
      "(7, array([[1.92598725]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"intent\": \"Procedure\",\n",
      "    \"rationale\": \"The message \"Submitted\" indicates an action taken in response to the assignment, thus relating to procedure.\"\n",
      "}\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     213.57 ms\n",
      "llama_print_timings:      sample time =       5.73 ms /    48 runs   (    0.12 ms per token,  8372.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44944.11 ms /  1536 tokens (   29.26 ms per token,    34.18 tokens per second)\n",
      "llama_print_timings:        eval time =    6972.18 ms /    48 runs   (  145.25 ms per token,     6.88 tokens per second)\n",
      "llama_print_timings:       total time =   52300.49 ms /  1584 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def memory_module(observations):\n",
    "    results = []\n",
    "    for indx, (obs) in enumerate(observations):\n",
    "        print(indx, observations[indx].__str__())\n",
    "        best = []\n",
    "        temp_prompt = class_prompt.replace(\"$MESSAGE$\", observations[indx].__str__()) #Replace message for \n",
    "        temp_prompt = temp_prompt.replace(\"s$TOPIC$\", topic) #Replace message for observation to classify\n",
    "        k_best_obs = query_k_obs(observations[indx].__str__(), 3, indx)\n",
    "        for i in k_best_obs:\n",
    "            best.append(i.__str__())\n",
    "        temp_prompt = temp_prompt.replace(\"$CONTEXT$\", \"\\n\".join(best))\n",
    "        result = classification(temp_prompt)\n",
    "        results.append(( observations[indx].value, result))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello. ', '```json\\n{\\n    \"intent\": \"Social\",\\n    \"rationale\": \"The message Hello. classifies as a social interaction.\"\\n}\\n```'), ('My assumption is', '```json\\n{\\n    \"intent\": \"Deliberation\",\\n    \"rationale\": \"The message \\\\\"My assumption is\\\\\" follows the initial discussion about an interpretation of the text (emphasis on barbarism), and as such it can be classified as a deliberation.\"\\n}\\n```'), ('that the emphasis on barbarism implies that she sent him to the lion.', '```json\\n{\\n    \"intent\": \"Deliberation\",\\n    \"rationale\": \"The message \"that the emphasis on barbarism implies that she sent him to the lion.\" relates to a deliberation as it discusses interpretation and meaning.\"\\n}\\n```'), (\"I agree with Cassandra's noticing \", '```json\\n{\\n    \"intent\": \"Deliberation\",\\n    \"rationale\": \"The message \\\\\"I agree with Cassandra\\'s noticing \\\\\" is a deliberation because it\\'s discussing the interpretation of content.\"\\n}\\n```'), (\"of the author's word choice of barbarism.\", '```json\\n{\\n    \"intent\": \"Seminar\",\\n    \"rationale\": \"The message \\\\\"of the author\\'s word choice of barbarism.\\\\\" relates to a seminar intent because it is discussing the meaning or interpretation of content.\"\\n}\\n```'), ('I loved the addition of ', '```json\\n{\\n    \"intent\": \"UX\",\\n    \"rationale\": \"The message \\'I loved the addition of \" \\' expresses the user\\'s opinion about the content or media which relates to UX intent.\"\\n}\\n```'), ('Her lover would die and never love another.\" ', '```json\\n{\\n    \"intent\": \"Seminar\",\\n    \"rationale\": \"The message \\'Her lover would die and never love another.\\' is an interpretation and discussion of the content.\"\\n}\\n```'), (' Sadly  the barbaric nature of the king and princess is still alive today in crimes of passion.\"', '```json\\n{\\n    \"intent\": \"Seminar\",\\n    \"rationale\": \"The message \\\\\"Sadly the barbaric nature of the king and princess is still alive today in crimes of passion\\\\\" reflects an interpretation and comparison to modern times, which fits into the Seminar intent.\"\\n}\\n```'), ('Submitted', '```json\\n{\\n    \"intent\": \"Procedure\",\\n    \"rationale\": \"The message \"Submitted\" indicates an action taken in response to the assignment, thus relating to procedure.\"\\n}\\n```')]\n"
     ]
    }
   ],
   "source": [
    "print (f_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Social', 'Seminar', 'Seminar', 'Seminar', 'Seminar', 'Seminar', 'Seminar', 'Seminar', 'Procedure']\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/frattitamayo/memory_module/CodingDiscourseAnalysis/CollabWriteAnalysisTest.xlsm'\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(file_path, sheet_name='Test', parse_dates=['Message Time'])\n",
    "y_true = []\n",
    "for index, row in df.iterrows():\n",
    "    val = row['R2DiscussionType']\n",
    "    y_true.append(val)\n",
    "print (y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = f_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[\"UX\", \"Social\", \"Procedure\", \"Deliberation\", \"Seminar\", \"Imaginative entry\", \"Disciplinary\", \"Other\"])\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=[\"UX\", \"Social\", \"Procedure\", \"Deliberation\", \"Seminar\", \"Imaginative entry\", \"Disciplinary\", \"Other\"], yticklabels=[\"UX\", \"Social\", \"Procedure\", \"Deliberation\", \"Seminar\", \"Imaginative entry\", \"Disciplinary\", \"Other\"])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Memory Module, class\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
